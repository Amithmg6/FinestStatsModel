{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "\"package 'sparklyr' was built under R version 4.0.2\"\n"
     ]
    }
   ],
   "source": [
    "library(sparklyr)\n",
    "\n",
    "sc=spark_connect(master = \"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error: java.lang.IllegalArgumentException: displ does not exist. Available: mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\r\n\tat org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)\r\n\tat scala.collection.MapLike.getOrElse(MapLike.scala:131)\r\n\tat scala.collection.MapLike.getOrElse$(MapLike.scala:129)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:63)\r\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:277)\r\n\tat org.apache.spark.ml.feature.RFormula.$anonfun$fit$3(RFormula.scala:223)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.immutable.List.map(List.scala:298)\r\n\tat org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:222)\r\n\tat org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:161)\r\n\tat org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)\r\n\tat org.apache.spark.ml.MLEvents.withFitEvent(events.scala:132)\r\n\tat org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:125)\r\n\tat org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)\r\n\tat org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.IterableViewLike$Transformed.foreach(IterableViewLike.scala:47)\r\n\tat scala.collection.IterableViewLike$Transformed.foreach$(IterableViewLike.scala:47)\r\n\tat scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:40)\r\n\tat org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)\r\n\tat org.apache.spark.ml.MLEvents.withFitEvent(events.scala:132)\r\n\tat org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:125)\r\n\tat org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)\r\n\tat org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)\r\n\tat org.apache.spark.ml.Pipeline.fit(Pipeline.scala:93)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sparklyr.Invoke.invoke(invoke.scala:147)\r\n\tat sparklyr.StreamHandler.handleMethodCall(stream.scala:136)\r\n\tat sparklyr.StreamHandler.read(stream.scala:61)\r\n\tat sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:58)\r\n\tat scala.util.control.Breaks.breakable(Breaks.scala:42)\r\n\tat sparklyr.BackendHandler.channelRead0(handler.scala:39)\r\n\tat sparklyr.BackendHandler.channelRead0(handler.scala:14)\r\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:321)\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:295)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\n",
     "output_type": "error",
     "traceback": [
      "Error: java.lang.IllegalArgumentException: displ does not exist. Available: mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\r\n\tat org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:278)\r\n\tat scala.collection.MapLike.getOrElse(MapLike.scala:131)\r\n\tat scala.collection.MapLike.getOrElse$(MapLike.scala:129)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:63)\r\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:277)\r\n\tat org.apache.spark.ml.feature.RFormula.$anonfun$fit$3(RFormula.scala:223)\r\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\r\n\tat scala.collection.immutable.List.foreach(List.scala:392)\r\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\r\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\r\n\tat scala.collection.immutable.List.map(List.scala:298)\r\n\tat org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:222)\r\n\tat org.apache.spark.ml.feature.RFormula.fit(RFormula.scala:161)\r\n\tat org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)\r\n\tat org.apache.spark.ml.MLEvents.withFitEvent(events.scala:132)\r\n\tat org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:125)\r\n\tat org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)\r\n\tat org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.IterableViewLike$Transformed.foreach(IterableViewLike.scala:47)\r\n\tat scala.collection.IterableViewLike$Transformed.foreach$(IterableViewLike.scala:47)\r\n\tat scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:40)\r\n\tat org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)\r\n\tat org.apache.spark.ml.MLEvents.withFitEvent(events.scala:132)\r\n\tat org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:125)\r\n\tat org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)\r\n\tat org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)\r\n\tat org.apache.spark.ml.Pipeline.fit(Pipeline.scala:93)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat sparklyr.Invoke.invoke(invoke.scala:147)\r\n\tat sparklyr.StreamHandler.handleMethodCall(stream.scala:136)\r\n\tat sparklyr.StreamHandler.read(stream.scala:61)\r\n\tat sparklyr.BackendHandler.$anonfun$channelRead0$1(handler.scala:58)\r\n\tat scala.util.control.Breaks.breakable(Breaks.scala:42)\r\n\tat sparklyr.BackendHandler.channelRead0(handler.scala:39)\r\n\tat sparklyr.BackendHandler.channelRead0(handler.scala:14)\r\n\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\r\n\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:321)\r\n\tat io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:295)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\r\n\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\r\n\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\r\n\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\r\n\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\r\n\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\r\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\r\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\r\n\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\r\n\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nTraceback:\n",
      "1. partitions$training %>% ml_linear_regression(hwy ~ displ + drv)",
      "2. withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))",
      "3. eval(quote(`_fseq`(`_lhs`)), env, env)",
      "4. eval(quote(`_fseq`(`_lhs`)), env, env)",
      "5. `_fseq`(`_lhs`)",
      "6. freduce(value, `_function_list`)",
      "7. withVisible(function_list[[k]](value))",
      "8. function_list[[k]](value)",
      "9. ml_linear_regression(., hwy ~ displ + drv)",
      "10. ml_linear_regression.tbl_spark(., hwy ~ displ + drv)",
      "11. ml_construct_model_supervised(new_ml_model_linear_regression, \n  .     predictor = stage, formula = formula, dataset = x, features_col = features_col, \n  .     label_col = label_col)",
      "12. ml_supervised_pipeline(predictor = predictor, dataset = dataset, \n  .     formula = formula, features_col = features_col, label_col = label_col)",
      "13. ml_pipeline(r_formula, predictor) %>% ml_fit(dataset)",
      "14. withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))",
      "15. eval(quote(`_fseq`(`_lhs`)), env, env)",
      "16. eval(quote(`_fseq`(`_lhs`)), env, env)",
      "17. `_fseq`(`_lhs`)",
      "18. freduce(value, `_function_list`)",
      "19. withVisible(function_list[[k]](value))",
      "20. function_list[[k]](value)",
      "21. ml_fit(., dataset)",
      "22. spark_jobj(x) %>% invoke(\"fit\", spark_dataframe(dataset)) %>% \n  .     ml_call_constructor()",
      "23. withVisible(eval(quote(`_fseq`(`_lhs`)), env, env))",
      "24. eval(quote(`_fseq`(`_lhs`)), env, env)",
      "25. eval(quote(`_fseq`(`_lhs`)), env, env)",
      "26. `_fseq`(`_lhs`)",
      "27. freduce(value, `_function_list`)",
      "28. function_list[[i]](value)",
      "29. invoke(., \"fit\", spark_dataframe(dataset))",
      "30. invoke.shell_jobj(., \"fit\", spark_dataframe(dataset))",
      "31. invoke_method(spark_connection(jobj), FALSE, jobj, method, ...)",
      "32. invoke_method.spark_shell_connection(spark_connection(jobj), \n  .     FALSE, jobj, method, ...)",
      "33. core_invoke_method(sc, static, object, method, ...)",
      "34. core_invoke_method_impl(sc, static, noreply = FALSE, object, \n  .     method, ...)",
      "35. withr::with_options(list(warning.length = 8000), {\n  .     if (nzchar(msg)) {\n  .         core_handle_known_errors(sc, msg)\n  .         stop(msg, call. = FALSE)\n  .     }\n  .     else {\n  .         msg <- core_read_spark_log_error(sc)\n  .         stop(msg, call. = FALSE)\n  .     }\n  . })",
      "36. force(code)",
      "37. stop(msg, call. = FALSE)"
     ]
    }
   ],
   "source": [
    "mpg=read.csv('datasets/mpg.csv')\n",
    "\n",
    "mpg_sc <- copy_to(sc,mpg)\n",
    "\n",
    "#partitions <- mpg_sc %>% \n",
    " # sdf_random_split(training = 0.8, test = 0.2)\n",
    "\n",
    "# model <- partitions$training %>% ml_linear_regression(hwy ~ displ + drv)\n",
    "\n",
    "# summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred <- ml_predict(model, partitions$test) %>% collect\n",
    "\n",
    "MSE <- mean((pred$hwy - pred$prediction)^2)\n",
    "\n",
    "MSE\n",
    "\n",
    "spark_disconnect(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error: 'diamonds' is not an exported object from 'namespace:datasets'\n",
     "output_type": "error",
     "traceback": [
      "Error: 'diamonds' is not an exported object from 'namespace:datasets'\nTraceback:\n",
      "1. load(datasets::diamonds)",
      "2. datasets::diamonds",
      "3. getExportedValue(pkg, name)",
      "4. stop(gettextf(\"'%s' is not an exported object from 'namespace:%s'\", \n .     name, getNamespaceName(ns)), call. = FALSE, domain = NA)"
     ]
    }
   ],
   "source": [
    "## fitting a linear regression model\n",
    "\n",
    "load(datasets::diamonds)\n",
    "\n",
    "diamonds.lm=lm(log(price)~log(carat),data = diamonds)\n",
    "\n",
    "summary(diamonds.lm)\n",
    "\n",
    "8.4486 + 1.6758 * 0.5\n",
    "\n",
    "exp(8.4486 + 1.6758 * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearity check\n",
    "\n",
    "plot(diamonds.lm, which = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(diamonds,aes(carat)) + geom_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(diamonds.lm, which = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(diamonds.lm, which = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(diamonds.lm, which = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(diamonds.lm, which = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(diamonds.lm, which = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds.lm2  <- lm(log(price) ~ log(carat) + x, data = diamonds)\n",
    "\n",
    "summary(diamonds.lm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
